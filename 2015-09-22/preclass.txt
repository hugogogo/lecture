0.  How much time did you spend on this pre-class exercise, and when?

1.  What are one or two points that you found least clear in the
    9/22 slide decks (including the narration)?

2.  The pthread_mc.c file in the demo subdirectory runs a Monte Carlo
    simulation to estimate the expected value of a uniform random
    variable.  The "-p" option sets the number of processors used,
    while "-b" sets the number of trials between synchronizations.

    a) Write a model for the run time for this simulation code in
       terms of the number of trials (N), batch size (B),
       number of processors (p), time per trial (t_trial), 
       and time to update the global counters in the critical 
       section (t_update).
	
	A:	computation = N * t_trial / p
		synchronization = N * t_update / B
		total = computation + synchronization

    b) Run the code with a few different parameter values in order
       to estimate N, t_trial, and t_update for this code on
       a totient compute node.
	
	A:	
		--- Run input parameters:
		rtol:      1.000000e-08
		maxtrials: 10000000000
		nbatch:    10000
		0.499998 (2.88671e-06) from 10000240000 trials
		24 threads (pthreads): 5.594676e+00 s
		--- Run input parameters:
		rtol:      1.000000e-08
		maxtrials: 10000000000
		nbatch:    20000
		0.499998 (2.88668e-06) from 10000480000 trials
		24 threads (pthreads): 5.580230e+00 s
		--- Run input parameters:
		rtol:      1.000000e-08
		maxtrials: 10000000000
		nbatch:    5000
		0.499996 (2.88673e-06) from 10000120000 trials
		24 threads (pthreads): 5.617796e+00 s

		From the results above,
		N = 10^10
		p = 24
		B = 10000, 20000, 5000

		So by a): we have
		10^10 * t_trial / 24 + 10^10 * t_update / 10000 = 5.595 s
		10^10 * t_trial / 24 + 10^10 * t_update / 20000 = 5.570 s
		10^10 * t_trial / 24 + 10^10 * t_update / 5000  = 5.618 s

		which implies that
		t_update = 2.312 * 10^(-8) s
		t_trial = 1.337 * 10^(-8) s

    c) Based on your model, suggest a strategy for choosing the batch
       size.  How might you generalize this strategy to automatically
       choose batch sizes for different types of computational
       experiments?
	
	A:	The general idea would be to increase batch size with the number of 
	processors since large batch size is good to amortize communication cost.
	But we cannot always choose as large as we want since too big a batch size
	means higher computational cost.

    
3.  In the workq subdirectory of this directory, there is a basic work
    queue implementation.  Following the strategy outlined in the
    slides, add synchronization calls in the locations marked TODO.
    You should run the code to make sure it behaves as expected!
	
	A: See code 
